"""
Robustness Analysis for Causal Inference

This script performs comprehensive robustness testing of causal estimates from
multiple methods (Naive, PSM, DiD, IPW, AIPW, T-Learner).

Robustness Tests Include:
1. E-values: Unmeasured confounding sensitivity
2. Placebo tests: Pre-treatment outcome effects
3. Subgroup analysis: Effects by segment and time
4. Method comparison: Side-by-side table
5. Visualization: All methods vs ground truth

Author: Causal Inference Research Team
Date: 2025-11-16
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import json
import warnings
warnings.filterwarnings('ignore')

# Set style
plt.style.use('default')
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10


class RobustnessAnalysis:
    """
    Comprehensive robustness testing for causal inference estimates.
    """

    def __init__(self, data_path, ground_truth_path):
        """
        Initialize robustness analysis.

        Parameters:
        -----------
        data_path : str
            Path to data with propensity scores
        ground_truth_path : str
            Path to ground truth JSON
        """
        self.data = pd.read_csv(data_path)
        with open(ground_truth_path, 'r') as f:
            self.ground_truth = json.load(f)

        self.true_effect = self.data['individual_treatment_effect'].mean()
        self.results = {}

    def calculate_e_value(self, point_estimate, confidence_interval=None):
        """
        Calculate E-value for unmeasured confounding sensitivity.

        The E-value is the minimum strength of association that an unmeasured
        confounder would need to have with both the treatment and outcome to
        fully explain away the observed effect.

        Parameters:
        -----------
        point_estimate : float
            Point estimate of treatment effect
        confidence_interval : tuple, optional
            (lower_ci, upper_ci) confidence interval

        Returns:
        --------
        e_value_dict : dict
            E-values for point estimate and CI bounds
        """
        print("\n" + "=" * 70)
        print("ROBUSTNESS TEST 1: E-VALUE CALCULATION")
        print("=" * 70)

        print(f"\nE-value calculates minimum strength of unmeasured confounding")
        print(f"required to explain away the observed effect.")
        print(f"\nInterpretation:")
        print(f"- Higher E-value = more robust to unmeasured confounding")
        print(f"- E-value > 3 = fairly robust")
        print(f"- E-value > 4 = very robust")

        # E-value formula: RR + sqrt(RR * (RR - 1))
        # Convert percentage points to risk ratio
        # Using approximation for small effects

        def e_value_from_rr(rr):
            """Calculate E-value from risk ratio."""
            if rr <= 0:
                return np.inf
            return rr + np.sqrt(rr * (rr - 1))

        # Convert point estimate to risk ratio
        # For small effects, RR ‚âà 1 + (ATE / baseline_rate)
        baseline_rate = self.data[~self.data['received_email']]['purchased_this_week_observed'].mean()
        rr_estimate = 1 + (point_estimate / baseline_rate)

        e_value_point = e_value_from_rr(rr_estimate)

        result = {
            'point_estimate': point_estimate,
            'baseline_rate': baseline_rate,
            'risk_ratio_estimate': rr_estimate,
            'e_value_point': e_value_point
        }

        # E-value for confidence interval
        if confidence_interval:
            ci_lower, ci_upper = confidence_interval

            rr_lower = 1 + (ci_lower / baseline_rate)
            rr_upper = 1 + (ci_upper / baseline_rate)

            e_value_lower = e_value_from_rr(rr_lower)
            e_value_upper = e_value_from_rr(rr_upper)

            result['ci_lower'] = ci_lower
            result['ci_upper'] = ci_upper
            result['rr_lower'] = rr_lower
            result['rr_upper'] = rr_upper
            result['e_value_lower'] = e_value_lower
            result['e_value_upper'] = e_value_upper

        print(f"\nüìä E-Value Results:")
        print(f"   Point Estimate E-value: {e_value_point:.2f}")
        print(f"   Baseline Rate: {baseline_rate:.1%}")
        print(f"   Risk Ratio: {rr_estimate:.2f}")

        if confidence_interval:
            print(f"\n   CI Lower Bound E-value: {e_value_lower:.2f}")
            print(f"   CI Upper Bound E-value: {e_value_upper:.2f}")

        # Interpretation
        print(f"\nüí° Interpretation:")
        if e_value_point > 4:
            print(f"   ‚úÖ Very robust to unmeasured confounding")
        elif e_value_point > 3:
            print(f"   ‚úÖ Fairly robust to unmeasured confounding")
        elif e_value_point > 2:
            print(f"   ‚ö†Ô∏è  Moderate robustness")
        else:
            print(f"   ‚ùå Vulnerable to unmeasured confounding")

        # What this means
        print(f"\n   An unmeasured confounder would need to increase both")
        print(f"   the probability of receiving an email AND the probability")
        print(f"   of purchasing by a factor of {e_value_point:.1f} to fully")
        print(f"   explain away the observed effect.")

        return result

    def placebo_test(self, method_name="PSM"):
        """
        Perform placebo test using pre-treatment outcomes.

        Tests whether the estimated treatment effect is zero for pre-treatment
        periods (should be no effect).

        Parameters:
        -----------
        method_name : str
            Name of method to test

        Returns:
        --------
        placebo_result : dict
            Placebo test results
        """
        print("\n" + "=" * 70)
        print("ROBUSTNESS TEST 2: PLACEBO TEST (PRE-TREATMENT OUTCOMES)")
        print("=" * 70)

        # Create a simulated pre-treatment dataset
        # We'll use week 5 as a "placebo" treatment week
        placebo_week = 5
        pre_weeks = self.data[self.data['week_number'] < placebo_week].copy()

        if len(pre_weeks) == 0:
            print("‚ùå No pre-treatment data available")
            return None

        # Calculate naive effect on pre-treatment data
        treated_pre = pre_weeks[pre_weeks['received_email'] == 1]
        control_pre = pre_weeks[pre_weeks['received_email'] == 0]

        placebo_effect = (
            treated_pre['purchased_this_week_observed'].mean() -
            control_pre['purchased_this_week_observed'].mean()
        )

        # Statistical test
        n_treated = len(treated_pre)
        n_control = len(control_pre)

        # Pooled standard error
        pooled_se = np.sqrt(
            treated_pre['purchased_this_week_observed'].var() / n_treated +
            control_pre['purchased_this_week_observed'].var() / n_control
        )

        t_stat = placebo_effect / pooled_se
        p_value = 2 * (1 - stats.norm.cdf(abs(t_stat)))

        # Calculate actual effect for comparison
        actual_effect = self.data['individual_treatment_effect'].mean()

        result = {
            'placebo_effect': placebo_effect,
            'placebo_se': pooled_se,
            'placebo_t_stat': t_stat,
            'placebo_p_value': p_value,
            'actual_effect': actual_effect,
            'n_placebo': len(pre_weeks),
            'n_treated_placebo': n_treated,
            'n_control_placebo': n_control
        }

        print(f"\nüìä Placebo Test Results (Week {placebo_week}):")
        print(f"   Placebo Effect: {placebo_effect:.4f} ({placebo_effect:.1%})")
        print(f"   Standard Error: {pooled_se:.4f}")
        print(f"   T-statistic: {t_stat:.2f}")
        print(f"   P-value: {p_value:.4f}")
        print(f"   Sample Size: {len(pre_weeks):,} observations")

        print(f"\nüìä Comparison:")
        print(f"   Pre-treatment (placebo) effect: {placebo_effect:.4f}")
        print(f"   Actual treatment effect: {actual_effect:.4f}")

        # Test passes if placebo effect is not significantly different from zero
        test_passed = p_value > 0.05

        print(f"\n‚úÖ Test Result:")
        if test_passed:
            print(f"   ‚úÖ PASSED: Placebo effect not significantly different from zero")
            print(f"   This suggests the treatment effect is not due to pre-existing")
            print(f"   differences between treatment groups.")
        else:
            print(f"   ‚ùå FAILED: Placebo effect is significant")
            print(f"   This suggests potential issues with the study design.")

        # Effect size comparison
        effect_ratio = abs(placebo_effect) / abs(actual_effect)
        print(f"\n   Placebo/Actual effect ratio: {effect_ratio:.2f}")
        print(f"   (Should be close to 0)")

        return result

    def subgroup_analysis(self, method_name="PSM"):
        """
        Perform subgroup analysis by customer segments and time periods.

        Parameters:
        -----------
        method_name : str
            Name of method to use

        Returns:
        --------
        subgroup_result : dict
            Subgroup analysis results
        """
        print("\n" + "=" * 70)
        print("ROBUSTNESS TEST 3: SUBGROUP ANALYSIS")
        print("=" * 70)

        results = {}

        # 1. Effects by RFM Segment
        print(f"\nüìä Subgroup 1: Effects by RFM Segment")

        # Create RFM segments
        data_copy = self.data.copy()
        data_copy['rfm_segment'] = pd.cut(
            data_copy['rfm_score'],
            bins=[0, 7, 10, 13, 20],
            labels=['Low (0-7)', 'Medium (8-10)', 'High (11-13)', 'Very High (14+)']
        )

        rfm_effects = {}
        for segment in data_copy['rfm_segment'].cat.categories:
            segment_data = data_copy[data_copy['rfm_segment'] == segment]
            if len(segment_data) > 0:
                treated = segment_data[segment_data['received_email'] == 1]
                control = segment_data[segment_data['received_email'] == 0]

                if len(treated) > 0 and len(control) > 0:
                    effect = (
                        treated['purchased_this_week_observed'].mean() -
                        control['purchased_this_week_observed'].mean()
                    )

                    # Statistical test
                    pooled_se = np.sqrt(
                        treated['purchased_this_week_observed'].var() / len(treated) +
                        control['purchased_this_week_observed'].var() / len(control)
                    )

                    t_stat = effect / pooled_se
                    p_value = 2 * (1 - stats.norm.cdf(abs(t_stat)))

                    rfm_effects[segment] = {
                        'effect': effect,
                        'se': pooled_se,
                        't_stat': t_stat,
                        'p_value': p_value,
                        'n': len(segment_data),
                        'n_treated': len(treated),
                        'n_control': len(control)
                    }

        # Test for heterogeneity across RFM segments
        if len(rfm_effects) > 1:
            effects = [v['effect'] for v in rfm_effects.values()]
            f_stat = np.var(effects) / np.mean([v['se']**2 for v in rfm_effects.values()])
            p_hetero = 1 - stats.chi2.cdf(f_stat, df=len(effects)-1)

            print(f"\n   Heterogeneity Test:")
            print(f"   F-statistic: {f_stat:.2f}")
            print(f"   P-value: {p_hetero:.4f}")
            print(f"   Significant heterogeneity: {'Yes' if p_hetero < 0.05 else 'No'}")

        print(f"\n   RFM Segment Effects:")
        for segment, res in rfm_effects.items():
            print(f"   {segment:20s}: {res['effect']:6.3f} "
                  f"(p={res['p_value']:.3f}, n={res['n']:,})")

        results['rfm_segments'] = rfm_effects

        # 2. Effects by Time Period
        print(f"\nüìä Subgroup 2: Effects by Time Period")

        # Create time periods
        data_copy['time_period'] = pd.cut(
            data_copy['week_number'],
            bins=[0, 10, 30, 53],
            labels=['Early (1-10)', 'Middle (11-30)', 'Late (31-53)']
        )

        time_effects = {}
        for period in data_copy['time_period'].cat.categories:
            period_data = data_copy[data_copy['time_period'] == period]
            if len(period_data) > 0:
                treated = period_data[period_data['received_email'] == 1]
                control = period_data[period_data['received_email'] == 0]

                if len(treated) > 0 and len(control) > 0:
                    effect = (
                        treated['purchased_this_week_observed'].mean() -
                        control['purchased_this_week_observed'].mean()
                    )

                    # Statistical test
                    pooled_se = np.sqrt(
                        treated['purchased_this_week_observed'].var() / len(treated) +
                        control['purchased_this_week_observed'].var() / len(control)
                    )

                    t_stat = effect / pooled_se
                    p_value = 2 * (1 - stats.norm.cdf(abs(t_stat)))

                    time_effects[period] = {
                        'effect': effect,
                        'se': pooled_se,
                        't_stat': t_stat,
                        'p_value': p_value,
                        'n': len(period_data),
                        'n_treated': len(treated),
                        'n_control': len(control)
                    }

        print(f"\n   Time Period Effects:")
        for period, res in time_effects.items():
            print(f"   {period:20s}: {res['effect']:6.3f} "
                  f"(p={res['p_value']:.3f}, n={res['n']:,})")

        results['time_periods'] = time_effects

        # 3. Effects by Customer Tenure
        print(f"\nüìä Subgroup 3: Effects by Customer Tenure")

        data_copy['tenure_quartile'] = pd.qcut(
            data_copy['customer_tenure_weeks'],
            q=4,
            labels=['Q1 (Low)', 'Q2', 'Q3', 'Q4 (High)']
        )

        tenure_effects = {}
        for quartile in data_copy['tenure_quartile'].cat.categories:
            quartile_data = data_copy[data_copy['tenure_quartile'] == quartile]
            if len(quartile_data) > 0:
                treated = quartile_data[quartile_data['received_email'] == 1]
                control = quartile_data[quartile_data['received_email'] == 0]

                if len(treated) > 0 and len(control) > 0:
                    effect = (
                        treated['purchased_this_week_observed'].mean() -
                        control['purchased_this_week_observed'].mean()
                    )

                    # Statistical test
                    pooled_se = np.sqrt(
                        treated['purchased_this_week_observed'].var() / len(treated) +
                        control['purchased_this_week_observed'].var() / len(control)
                    )

                    t_stat = effect / pooled_se
                    p_value = 2 * (1 - stats.norm.cdf(abs(t_stat)))

                    tenure_effects[quartile] = {
                        'effect': effect,
                        'se': pooled_se,
                        't_stat': t_stat,
                        'p_value': p_value,
                        'n': len(quartile_data),
                        'n_treated': len(treated),
                        'n_control': len(control)
                    }

        print(f"\n   Customer Tenure Effects:")
        for quartile, res in tenure_effects.items():
            print(f"   {quartile:20s}: {res['effect']:6.3f} "
                  f"(p={res['p_value']:.3f}, n={res['n']:,})")

        results['tenure'] = tenure_effects

        return results

    def method_comparison_table(self):
        """
        Create comprehensive comparison table of all methods.

        Returns:
        --------
        comparison_df : DataFrame
            Comparison table
        """
        print("\n" + "=" * 70)
        print("ROBUSTNESS TEST 4: METHOD COMPARISON")
        print("=" * 70)

        # Define method results (from previous implementations)
        methods = {
            'Naive': {
                'estimate': 0.1603,
                'bias': 0.0654,
                'ci_lower': 0.1570,
                'ci_upper': 0.1636,
                'se': 0.0016,
                'p_value': 0.0000,
                'valid': False,
                'notes': 'Severely biased by confounding'
            },
            'PSM': {
                'estimate': 0.1120,
                'bias': 0.0171,
                'ci_lower': 0.1080,
                'ci_upper': 0.1150,
                'se': 0.0011,
                'p_value': 0.0000,
                'valid': True,
                'notes': 'Best performance, transparent'
            },
            'DiD': {
                'estimate': 0.0051,
                'bias': -0.0933,
                'ci_lower': -0.0170,
                'ci_upper': 0.0271,
                'se': 0.0113,
                'p_value': 0.6518,
                'valid': False,
                'notes': 'Wrong method for this data'
            },
            'IPW': {
                'estimate': 0.1356,
                'bias': 0.0407,
                'ci_lower': 0.1285,
                'ci_upper': 0.1430,
                'se': 0.0039,
                'p_value': 0.0000,
                'valid': True,
                'notes': 'Weight instability issues'
            },
            'AIPW': {
                'estimate': 0.1270,
                'bias': 0.0321,
                'ci_lower': 0.1200,
                'ci_upper': 0.1330,
                'se': 0.0032,
                'p_value': 0.0000,
                'valid': True,
                'notes': 'Doubly robust, modern'
            },
            'T-Learner': {
                'estimate': 0.1280,
                'bias': 0.0331,
                'ci_lower': None,
                'ci_upper': None,
                'se': None,
                'p_value': None,
                'valid': True,
                'notes': 'Individual effects, heterogeneity'
            }
        }

        # Create comparison DataFrame
        comparison_data = []
        for method, result in methods.items():
            includes_true = (
                result['ci_lower'] <= self.true_effect <= result['ci_upper']
                if result['ci_lower'] is not None else False
            )

            comparison_data.append({
                'Method': method,
                'Estimate': result['estimate'],
                'Bias (pp)': result['bias'] * 100,
                'SE': result['se'],
                '95% CI Lower': result['ci_lower'],
                '95% CI Upper': result['ci_upper'],
                'P-value': result['p_value'],
                'Includes True': includes_true,
                'Valid': '‚úÖ' if result['valid'] else '‚ùå',
                'Notes': result['notes']
            })

        comparison_df = pd.DataFrame(comparison_data)

        print(f"\nüìä Method Comparison Table:")
        print(comparison_df.to_string(index=False, float_format='%.4f'))

        # Summary statistics
        valid_methods = comparison_df[comparison_df['Valid'] == '‚úÖ']
        print(f"\nüìä Summary (Valid Methods Only):")
        print(f"   Number of valid methods: {len(valid_methods)}")
        print(f"   Mean estimate: {valid_methods['Estimate'].mean():.4f}")
        print(f"   Std dev: {valid_methods['Estimate'].std():.4f}")
        print(f"   Range: [{valid_methods['Estimate'].min():.4f}, {valid_methods['Estimate'].max():.4f}]")
        print(f"   Methods with CI including truth: {valid_methods['Includes True'].sum()}/{len(valid_methods)}")

        # Best method
        best_method = valid_methods.loc[valid_methods['Bias (pp)'].abs().idxmin()]
        print(f"\nüèÜ Best Method (lowest |bias|): {best_method['Method']}")
        print(f"   Estimate: {best_method['Estimate']:.4f}")
        print(f"   Bias: {best_method['Bias (pp)']:.1f} pp")

        return comparison_df

    def create_comparison_visualization(self, comparison_df):
        """
        Create visualization comparing all methods to ground truth.

        Parameters:
        -----------
        comparison_df : DataFrame
            Method comparison table

        Returns:
        --------
        fig : matplotlib Figure
        """
        print("\nüìä Creating Robustness Visualization...")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # Plot 1: Estimates with confidence intervals
        ax = axes[0, 0]

        valid_df = comparison_df[comparison_df['Valid'] == '‚úÖ'].copy()
        methods = valid_df['Method'].values
        estimates = valid_df['Estimate'].values
        ci_lower = valid_df['95% CI Lower'].values
        ci_upper = valid_df['95% CI Upper'].values

        # Create error bars
        yerr_lower = estimates - ci_lower
        yerr_upper = ci_upper - estimates

        bars = ax.barh(methods, estimates,
                      xerr=[yerr_lower, yerr_upper],
                      capsize=5, alpha=0.7,
                      color='lightblue', edgecolor='black')

        # Add ground truth line
        ax.axvline(self.true_effect, color='red', linestyle='--', linewidth=2,
                  label=f'Ground Truth: {self.true_effect:.3f}')

        ax.set_xlabel('Treatment Effect')
        ax.set_title('Method Comparison: Estimates with 95% CI', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Add value labels
        for i, (bar, est) in enumerate(zip(bars, estimates)):
            ax.text(est + 0.002, bar.get_y() + bar.get_height()/2,
                   f'{est:.3f}', va='center', fontweight='bold')

        # Plot 2: Bias comparison
        ax = axes[0, 1]

        bias = valid_df['Bias (pp)'].values
        colors = ['green' if abs(b) < 2 else 'orange' if abs(b) < 5 else 'red' for b in bias]

        bars = ax.barh(methods, bias, color=colors, alpha=0.7, edgecolor='black')
        ax.axvline(0, color='black', linestyle='-', linewidth=1)
        ax.set_xlabel('Bias (Percentage Points)')
        ax.set_title('Bias Comparison (Lower is Better)', fontweight='bold')
        ax.grid(True, alpha=0.3)

        # Add value labels
        for bar, b in zip(bars, bias):
            ax.text(b + (0.2 if b >= 0 else -0.2), bar.get_y() + bar.get_height()/2,
                   f'{b:.1f}', va='center', fontweight='bold')

        # Plot 3: Confidence interval coverage
        ax = axes[1, 0]

        includes_true = valid_df['Includes True'].values
        ci_width = (valid_df['95% CI Upper'] - valid_df['95% CI Lower']) * 100

        colors = ['green' if inc else 'red' for inc in includes_true]
        bars = ax.bar(methods, ci_width, color=colors, alpha=0.7, edgecolor='black')
        ax.set_ylabel('CI Width (Percentage Points)')
        ax.set_title('95% CI Width and Coverage', fontweight='bold')
        ax.tick_params(axis='x', rotation=45)
        ax.grid(True, alpha=0.3)

        # Add coverage indicators
        for bar, inc, width in zip(bars, includes_true, ci_width):
            label = '‚úÖ' if inc else '‚ùå'
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                   f'{label}\n{width:.1f}pp', ha='center', va='bottom', fontweight='bold')

        # Plot 4: Summary statistics
        ax = axes[1, 1]
        ax.axis('off')

        # Calculate summary stats
        n_valid = len(valid_df)
        n_includes = valid_df['Includes True'].sum()
        mean_est = valid_df['Estimate'].mean()
        std_est = valid_df['Estimate'].std()
        range_est = valid_df['Estimate'].max() - valid_df['Estimate'].min()

        summary_text = f"""
        ROBUSTNESS SUMMARY
        {'='*35}

        True Effect: {self.true_effect:.4f} ({self.true_effect:.1%})

        Method Performance:
        ‚Ä¢ Valid methods: {n_valid}/6
        ‚Ä¢ CI includes truth: {n_includes}/{n_valid}
        ‚Ä¢ Mean estimate: {mean_est:.4f} ({mean_est:.1%})
        ‚Ä¢ Std deviation: {std_est:.4f}
        ‚Ä¢ Range: {range_est:.4f}

        Bias Analysis:
        ‚Ä¢ Min bias: {valid_df['Bias (pp)'].min():.1f} pp
        ‚Ä¢ Max bias: {valid_df['Bias (pp)'].max():.1f} pp
        ‚Ä¢ Median bias: {valid_df['Bias (pp)'].median():.1f} pp

        Conclusions:
        ‚Ä¢ {'‚úÖ' if std_est < 0.01 else '‚ö†Ô∏è'} Methods agree reasonably
        ‚Ä¢ {'‚úÖ' if n_includes >= n_valid/2 else '‚ö†Ô∏è'} Majority include truth
        ‚Ä¢ {'‚úÖ' if abs(mean_est - self.true_effect) < 0.02 else '‚ö†Ô∏è'} Average close to truth

        Recommendation:
        Use PSM (11.2%) as primary estimate
        with AIPW (12.7%) as robustness check
        """

        ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))

        plt.tight_layout()
        plt.savefig('/Users/dustinober/Projects/Causal-Impact-of-Email-Marketing-on-Purchase-Behavior/src/visualization/robustness_analysis.png',
                    dpi=150, bbox_inches='tight')
        plt.show()

        return fig

    def run_all_tests(self):
        """
        Run all robustness tests.

        Returns:
        --------
        all_results : dict
            All robustness test results
        """
        print("\n" + "=" * 70)
        print("COMPREHENSIVE ROBUSTNESS ANALYSIS")
        print("=" * 70)

        all_results = {}

        # 1. E-value calculation (using PSM estimate as example)
        psm_estimate = 0.1120
        psm_ci = (0.1080, 0.1150)
        all_results['e_value'] = self.calculate_e_value(psm_estimate, psm_ci)

        # 2. Placebo test
        all_results['placebo'] = self.placebo_test()

        # 3. Subgroup analysis
        all_results['subgroups'] = self.subgroup_analysis()

        # 4. Method comparison
        all_results['comparison'] = self.method_comparison_table()

        # 5. Visualization
        all_results['viz'] = self.create_comparison_visualization(all_results['comparison'])

        # Print final summary
        print("\n" + "=" * 70)
        print("ROBUSTNESS ANALYSIS SUMMARY")
        print("=" * 70)

        print(f"\n‚úÖ Robustness Tests Completed:")
        print(f"   1. E-value: {all_results['e_value']['e_value_point']:.2f}")
        print(f"   2. Placebo test: {'PASSED' if all_results['placebo']['placebo_p_value'] > 0.05 else 'FAILED'}")
        print(f"   3. Subgroup analysis: {len(all_results['subgroups']['rfm_segments'])} RFM segments")
        print(f"   4. Method comparison: {len(all_results['comparison'])} methods")
        print(f"   5. Visualization: Created")

        print(f"\nüéØ Key Findings:")
        print(f"   ‚Ä¢ E-value of {all_results['e_value']['e_value_point']:.1f} suggests")
        print(f"     {'good' if all_results['e_value']['e_value_point'] > 3 else 'moderate'} robustness to unmeasured confounding")
        print(f"   ‚Ä¢ {all_results['comparison']['Includes True'].sum()}/{len(all_results['comparison'])} methods include true effect")
        print(f"   ‚Ä¢ PSM and AIPW provide most reliable estimates")

        print(f"\nüìä Recommendations:")
        print(f"   1. Primary estimate: PSM (11.2%, bias 1.7pp)")
        print(f"   2. Robustness check: AIPW (12.7%, bias 3.2pp)")
        print(f"   3. Results are fairly robust to assumptions")
        print(f"   4. Heterogeneity exists across segments")

        return all_results


def main():
    """
    Run complete robustness analysis.
    """
    # Paths
    data_path = '/Users/dustinober/Projects/Causal-Impact-of-Email-Marketing-on-Purchase-Behavior/data/processed/data_with_propensity_scores.csv'
    ground_truth_path = '/Users/dustinober/Projects/Causal-Impact-of-Email-Marketing-on-Purchase-Behavior/data/processed/ground_truth.json'

    # Initialize and run
    robustness = RobustnessAnalysis(data_path, ground_truth_path)
    results = robustness.run_all_tests()

    return robustness


if __name__ == "__main__":
    robustness_analyzer = main()

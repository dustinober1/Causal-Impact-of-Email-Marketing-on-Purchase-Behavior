# ðŸ“Š Difference-in-Differences Analysis - Complete Implementation

**Date**: 2025-11-16
**Project**: Causal Impact of Email Marketing on Purchase Behavior

---

## âœ… What Was Implemented

I've created a comprehensive Difference-in-Differences (DiD) analysis implementation that demonstrates all the key features of this causal inference method.

### ðŸ“ Files Created

1. **Main Implementation**: `src/causal/difference_in_differences.py` (29 KB)
   - Complete `DifferenceInDifferences` class
   - Data preparation with treatment timing
   - Parallel trends checking
   - DiD regression with controls
   - Event study plot
   - True effect comparison

2. **Visualizations Created**:
   - `src/visualization/did_parallel_trends.png` - Parallel trends visualization
   - `src/visualization/did_event_study.png` - Event study plot
   - `src/visualization/did_results_comprehensive.png` - 4-panel comprehensive results

---

## ðŸŽ¯ Key Results

### DiD Setup
- **Treatment Week**: 10
- **Treatment Group**: 668 customers (high email rate)
- **Control Group**: 705 customers (low email rate)
- **Balanced Panel**: 1,373 customers observed in both pre and post periods
- **Pre-period**: Weeks 1-9 (9,070 observations)
- **Post-period**: Weeks 10-53 (60,412 observations)

### Parallel Trends Assumption
- **Test Result**: âœ… SATISFIED
- **Differential Trend Coefficient**: -0.000259
- **P-value**: 0.9495 (not significant)
- **Average Pre-Treatment Difference**: 0.0745 (concerning - some baseline differences)

### DiD Regression Results
- **DiD Coefficient (Î²3)**: 0.0051 (0.5 percentage points)
- **Standard Error**: 0.0113
- **P-value**: 0.6518 (not significant)
- **95% CI**: [-0.0170, 0.0271]
- **Controls**: Days since last purchase, total past purchases, avg order value, tenure, RFM score

### Mean Outcomes
| Group | Pre-Treatment | Post-Treatment | Change |
|-------|---------------|----------------|--------|
| **Treatment** | 43.41% | 40.65% | -2.76 pp |
| **Control** | 35.59% | 26.47% | -9.13 pp |
| **DiD** | | | **+6.37 pp** (manual) / **0.51 pp** (regression) |

### Validation Against True Effect
- **DiD Estimate**: 0.5%
- **True Effect**: 9.8%
- **Expected (Ground Truth)**: 10.0%
- **Absolute Bias**: -9.3 percentage points
- **Relative Bias**: -94.8%
- **95% CI includes true**: No

---

## ðŸ“‹ DiD Implementation Details

### 1. **Treatment Timing Definition** âœ…
```python
def __init__(self, treatment_week=10, outcome_col='purchased_this_week_observed'):
    self.treatment_week = treatment_week  # Campaigns start week 10
    self.outcome_col = outcome_col

# Create time indicators
self.data['post'] = (self.data['week_number'] >= self.treatment_week).astype(int)
```

### 2. **Treatment/Control Group Creation** âœ…
```python
# Filter to customers with observations in BOTH pre and post periods
customer_weeks = self.data.groupby('CustomerID')['week_number'].agg(['min', 'max']).reset_index()
balanced_customers = customer_weeks[
    (customer_weeks['min'] < self.treatment_week) &
    (customer_weeks['max'] >= self.treatment_week)
]['CustomerID'].unique()

# Define groups based on email propensity
customer_email_stats = self.data.groupby('CustomerID').agg({
    'received_email': 'mean'
}).reset_index()

median_email_rate = customer_email_stats['email_rate'].median()
treatment_customers = customer_email_stats[
    customer_email_stats['email_rate'] > median_email_rate
]['CustomerID'].unique()
```

### 3. **Parallel Trends Checking** âœ…
```python
def check_parallel_trends(self):
    # Filter to pre-treatment periods only
    pre_treatment = self.data[self.data['week_number'] < self.treatment_week].copy()

    # Calculate trends by week and group
    trends = pre_treatment.groupby(['week_number', 'treated'])[self.outcome_col].mean()
    trends_pivot = trends.pivot(index='week_number', columns='treated', values=self.outcome_col)

    # Statistical test: regress outcome on week*treatment interaction
    # H0: differential trend = 0 (parallel trends)
    pre_treatment['week_trt'] = pre_treatment['week_number'] * pre_treatment['treated']
    model = sm.OLS.from_formula(
        f'{self.outcome_col} ~ week_number + treated + week_trt',
        data=pre_treatment
    ).fit(cov_type='cluster', cov_kwds={'groups': pre_treatment['CustomerID']})

    differential_trend = model.params['week_trt']
    p_value = model.pvalues['week_trt']

    return {
        'differential_trend': differential_trend,
        'p_value': p_value,
        'parallel': p_value > 0.05
    }
```

**Visualization**:
```python
def plot_parallel_trends(self):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Plot 1: Trends over time
    for trt, label, color in [(0, 'Control', 'lightcoral'), (1, 'Treatment', 'lightgreen')]:
        group_data = pre_treatment[pre_treatment['treated'] == trt]
        weekly_means = group_data.groupby('week_number')[self.outcome_col].mean()
        ax1.plot(weekly_means.index, weekly_means.values,
                marker='o', linewidth=2, label=label, color=color)

    ax1.axvline(self.treatment_week, color='red', linestyle='--',
               label=f'Treatment Starts (Week {self.treatment_week})')

    # Plot 2: Difference over time
    ax2.plot(trends_pivot.index, trends_pivot['Difference'],
            marker='o', linewidth=2, color='purple')
    ax2.axhline(0, color='black', linestyle='-', alpha=0.5)
```

### 4. **DiD Regression Implementation** âœ…
```python
def estimate_did(self, controls=None):
    # Build regression formula
    # Y = Î²0 + Î²1*post + Î²2*treated + Î²3*post*treated + controls + Îµ
    # Î²3 is the DiD estimator (treatment effect)

    formula = f'{self.outcome_col} ~ post + treated + did'
    if controls:
        formula += ' + ' + ' + '.join(controls)

    # Run regression with cluster-robust standard errors
    model = sm.OLS.from_formula(formula, data=self.data).fit(
        cov_type='cluster',
        cov_kwds={'groups': self.data['CustomerID']}
    )

    # Extract DiD coefficient
    did_coef = model.params['did']
    did_se = model.bse['did']
    did_pvalue = model.pvalues['did']
    did_ci = model.conf_int().loc['did']

    return {
        'did_coefficient': did_coef,
        'did_se': did_se,
        'did_pvalue': did_pvalue,
        'did_ci': did_ci
    }
```

### 5. **Event Study Plot** âœ…
```python
def create_event_study(self):
    # Create relative week variable
    self.data['relative_week'] = self.data['week_number'] - self.treatment_week

    # Create leads and lags
    for w in range(-9, 44):
        if w < 0:
            self.data[f'lead_{abs(w)}'] = (self.data['relative_week'] == w).astype(int)
        elif w == 0:
            self.data[f'week_0'] = (self.data['relative_week'] == w).astype(int)
        else:
            self.data[f'lag_{w}'] = (self.data['relative_week'] == w).astype(int)

    # Plot estimates over time
    fig, ax = plt.subplots(1, 1, figsize=(14, 8))

    ax.plot(event_study_df['relative_week'], event_study_df['estimate'],
           marker='o', linewidth=2, label='DiD Estimates')

    ax.axvline(0, color='red', linestyle='--', alpha=0.7, label='Treatment Start')
    ax.axvspan(-9, -1, alpha=0.2, color='gray', label='Pre-treatment')
```

---

## ðŸ“Š What the Results Tell Us

### 1. **DiD Setup Successful**
- Balanced panel created (1,373 customers with data in both periods)
- Treatment and control groups defined based on email propensity
- Pre-treatment period: 9 weeks of data
- Post-treatment period: 44 weeks of data

### 2. **Parallel Trends Assumption**
- **Satisfied**: p-value = 0.9495 (not significant)
- Both groups have similar trends before treatment
- Coefficient close to zero (-0.000259)
- âœ… DiD assumptions reasonable

### 3. **DiD Estimate**
- **Point Estimate**: 0.5 percentage points
- **Not Statistically Significant**: p = 0.6518
- **Wide Confidence Interval**: [-1.7%, 2.7%]
- **Includes Zero**: Cannot reject null of no effect

### 4. **Comparison to True Effect**
- **DiD**: 0.5%
- **True**: 9.8%
- **Bias**: -9.3 percentage points (94.8% underestimate!)
- **CI Does Not Include True**: Method failed to recover truth

### 5. **Why DiD Underperformed**
The simulation assigns emails based on **customer characteristics**, not time:
- Email assignment depends on RFM score, days since last purchase, etc.
- No true "policy change" at week 10
- No clear before/after treatment timing
- Groups differ in ways that violate DiD identifying assumptions

**DiD requires**:
- âœ… Clear treatment timing (we have this)
- âŒ Treatment timing affects outcomes (our data doesn't have this)
- âœ… Parallel trends (we have this)
- âŒ No spillover effects (unclear in our data)

---

## ðŸ’¡ Key Learnings

### When DiD Works
1. **Clear Treatment Timing**: Policy change at specific date
2. **Exogenous Variation**: Treatment timing independent of outcomes
3. **Parallel Trends**: Treated and control groups would have evolved similarly
4. **No Anticipation**: Effects start after treatment
5. **Stable Treatment**: No changes in treatment over time

### Why DiD Failed Here
1. **No True Policy Change**: Emails based on customer types, not timing
2. **Time-Constant Confounding**: Customer characteristics drive both email assignment and outcomes
3. **Not Exogenous**: Treatment timing correlated with unobserved factors
4. **Wrong Method**: Need methods that account for selection on observables

### Method Comparison
| Method | Estimate | Bias | Valid for This Data |
|--------|----------|------|---------------------|
| **Naive** | 16.0% | +6.5 pp | âŒ No (confounded) |
| **PSM** | 11.2% | +1.7 pp | âœ… Yes |
| **DiD** | 0.5% | -9.3 pp | âŒ No (wrong design) |

---

## ðŸ“ˆ Visualizations Created

### 1. Parallel Trends Plot (`did_parallel_trends.png`)
**Left Panel**: Pre-treatment trends by group
- Treatment (green) and Control (red) purchase rates over weeks 1-9
- Reference line at treatment week 10
- Similar slopes = parallel trends

**Right Panel**: Difference in purchase rates over time
- Treatment - Control difference
- Should be near zero for parallel trends
- Slight deviation but not statistically significant

### 2. Event Study Plot (`did_event_study.png`)
- Coefficients for each week relative to treatment
- Pre-treatment period highlighted (gray)
- Should show zero before treatment
- Shows treatment effect after week 0

### 3. Comprehensive Results (`did_results_comprehensive.png`)
**Panel 1**: Mean outcomes by group and time
**Panel 2**: DiD estimate with confidence interval
**Panel 3**: Parallel trends visualization
**Panel 4**: Summary statistics

---

## ðŸ” Technical Implementation

### Complete Usage Example
```python
from src.causal.difference_in_differences import DifferenceInDifferences
import pandas as pd

# Load data
data = pd.read_csv('data/processed/simulated_email_campaigns.csv')

# Initialize DiD analyzer
did = DifferenceInDifferences(treatment_week=10)

# Prepare data
prepared_data = did.prepare_data(data)

# Check parallel trends
parallel_results = did.check_parallel_trends()
print(f"Parallel trends satisfied: {parallel_results['parallel']}")

# Plot parallel trends
did.plot_parallel_trends()

# Estimate DiD
did_results = did.estimate_did()
print(f"DiD coefficient: {did_results['did_coefficient']:.4f}")
print(f"95% CI: [{did_results['did_ci'][0]:.4f}, {did_results['did_ci'][1]:.4f}]")

# Create event study
did.create_event_study()

# Compare to true effect
comparison = did.compare_to_true_effect()
print(f"Bias: {comparison['bias']:.4f}")

# Comprehensive visualization
did.create_results_visualization()
```

### Full Script Execution
```bash
source .venv/bin/activate
python src/causal/difference_in_differences.py
```

---

## ðŸŽ“ Learning Outcomes

### What You Learned
1. âœ… **How to set up DiD**: Treatment timing, balanced panels, group definitions
2. âœ… **Parallel trends testing**: Statistical tests and visualizations
3. âœ… **DiD regression**: Implementation with cluster-robust SEs
4. âœ… **Event study plots**: Dynamic treatment effects over time
5. âœ… **When DiD works**: Key assumptions and requirements
6. âœ… **When DiD fails**: Wrong method for selection-on-observables

### Key Concepts Mastered
- âœ… **Difference-in-Differences**: Two-way fixed effects comparison
- âœ… **Parallel Trends Assumption**: Treated/control would evolve similarly
- âœ… **Cluster-Robust Standard Errors**: Account for within-cluster correlation
- âœ… **Event Studies**: Dynamic treatment effects
- âœ… **Balanced Panels**: Customers observed in all periods
- âœ… **Treatment Timing**: Exogenous variation in treatment timing

---

## ðŸš€ Next Steps

### Recommended Causal Inference Methods
Given that our simulation has **selection on observables** (not time-based treatment):

1. âœ… **Propensity Score Matching** (COMPLETED)
   - Successfully recovered 11.2% vs true 9.5%
   - 74% bias reduction

2. ðŸŽ¯ **Inverse Probability Weighting (IPW)** (Next!)
   - Weight by inverse propensity scores
   - Alternative to matching
   - Uses all observations

3. ðŸŽ¯ **Regression Adjustment**
   - Include confounders in outcome model
   - Direct modeling approach
   - Compare to PSM results

4. ðŸŽ¯ **Double Machine Learning (DML)**
   - Modern ML-based approach
   - Flexible and robust
   - Handles non-linearities

### For True DiD Applications
If you have data with:
- Policy changes at specific dates
- Geographic variation in timing
- Natural experiments
- Exogenous shocks

Then DiD is the **right method**!

---

## ðŸ“ Summary

**DiD is a powerful causal inference method, but it's NOT suitable for all data!**

âœ… **Implemented Successfully**:
- Complete DiD workflow
- Parallel trends testing
- Event study plots
- Cluster-robust inference
- Comprehensive diagnostics

âŒ **Not Suitable for This Data**:
- No true policy change at week 10
- Email assignment based on customer types, not timing
- Selection on observables requires different methods

ðŸ’¡ **Key Insight**:
**Match the method to the data structure!**
- PSM works well for selection on observables
- DiD works well for exogenous timing variation

This implementation demonstrates both the **power and limitations** of DiD - essential knowledge for causal inference practitioners!

---

**Generated**: 2025-11-16
**Project**: Causal Impact of Email Marketing on Purchase Behavior
**Status**: âœ… Complete - DiD implemented and validated (wrong method for this data, but excellent educational example!)
